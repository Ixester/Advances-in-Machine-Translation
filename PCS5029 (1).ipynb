{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXijEYPuaCJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg3pjs_uTvMO"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Dicionário de tradução (exemplo simplificado)\n",
        "translation_dict = {\n",
        "    'is': 'é',\n",
        "    'house': 'casa',\n",
        "    'small': 'pequena',\n",
        "    'The':'a',\n",
        "}\n",
        "\n",
        "def translate_sentence(sentence, translation_dict):\n",
        "    translated_words = [translation_dict.get(word, word) for word in sentence.split()]\n",
        "    translated_sentence = ' '.join(translated_words)\n",
        "    return translated_sentence\n",
        "\n",
        "input_sentence = \"The house is small\"\n",
        "translated_sentence = translate_sentence(input_sentence, translation_dict)\n",
        "print(translated_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de um modelo Seq2Seq com LSTM\n"
      ],
      "metadata": {
        "id": "SaLqPUUFxE17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Preparação dos dados de treinamento\n",
        "input_sequence = np.array([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n",
        "output_sequence = np.array([[2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]])\n",
        "\n",
        "# Tamanhos dos vocabulários de entrada e saída\n",
        "input_vocab_size = 10\n",
        "output_vocab_size = 10\n",
        "\n",
        "# Tamanho da camada de embedding e unidades LSTM\n",
        "embedding_dim = 32\n",
        "units = 64\n",
        "\n",
        "# Camada de entrada\n",
        "inputs = Input(shape=(4,))\n",
        "embedding_layer = Embedding(input_dim=input_vocab_size, output_dim=embedding_dim)(inputs)\n",
        "\n",
        "# Camada LSTM (encoder)\n",
        "encoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(embedding_layer)\n",
        "\n",
        "# Camada de saída (decoder)\n",
        "outputs = Dense(output_vocab_size, activation='softmax')(encoder_outputs)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Ajuste o formato dos dados de saída para one-hot encoding\n",
        "output_sequence_onehot = tf.one_hot(output_sequence, depth=output_vocab_size)\n",
        "\n",
        "model.fit(input_sequence, output_sequence_onehot, epochs=100)\n",
        "\n",
        "# Gere previsões\n",
        "input_sequence_test = np.array([[4, 5, 6, 7]])\n",
        "predicted_output = model.predict(input_sequence_test)\n",
        "print(predicted_output)"
      ],
      "metadata": {
        "id": "Nqe2W-YoVQaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exemplo de um modelo NMT que usa arquitetura Transformer"
      ],
      "metadata": {
        "id": "icGzheN93ftk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zimport tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Preparação dos dados de treinamento\n",
        "input_sequence = np.array([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n",
        "output_sequence = np.array([[2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]])\n",
        "\n",
        "# Tamanhos dos vocabulários de entrada e saída\n",
        "input_vocab_size = 10\n",
        "output_vocab_size = 10\n",
        "\n",
        "# Tamanho da camada de embedding e unidades\n",
        "embedding_dim = 32\n",
        "num_heads = 2\n",
        "\n",
        "# Camada de entrada\n",
        "inputs = Input(shape=(4,))\n",
        "embedding_layer = Embedding(input_dim=input_vocab_size, output_dim=embedding_dim)(inputs)\n",
        "\n",
        "# Camada de atenção multihead (Transformer)\n",
        "attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(embedding_layer, embedding_layer)\n",
        "output = Dense(output_vocab_size, activation='softmax')(attention)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Ajuste o formato dos dados de saída para one-hot encoding\n",
        "output_sequence_onehot = tf.one_hot(output_sequence, depth=output_vocab_size)\n",
        "\n",
        "model.fit(input_sequence, output_sequence_onehot, epochs=100)\n",
        "\n",
        "# Gere previsões\n",
        "input_sequence_test = np.array([[4, 5, 6, 7]])\n",
        "predicted_output = model.predict(input_sequence_test)\n",
        "print(predicted_output)\n"
      ],
      "metadata": {
        "id": "Zdxm11Hz1-6j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}